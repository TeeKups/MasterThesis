With the growth in computing power and sensor capability,
new possibilities are opened for human activity recognition.
Multiple applications for human activity recognition have been proposed,
such as health case and shoplifting recognition.
Some commercial applications are already available,
such as the health and fitness applications found in smart phones and watches.

Typically activity recognition is performed on video or wearable sensors.
Alas, video based sensing is susceptible to lightning conditions and obstructions,
whereas worn sensors may be cumbersome, unfashionable, or otherwise inconvenient.

Clearly, there is demand for alternative mechanisms for human activity recognition.
Additionally, sensor fusion has grown in popularity in the recent years.
Sensors can only sense certain modalities,
thus combining multi-modal sensors may bring improvements to activity recognition accuracy.

Data sets that consider sensor fusion are unfortunately few.
For this reason, it was seen valuable to develop a recording system
that combines multiple sensors.
The sensors chosen for the system were an RGB-D camera, 60 GHz radar, low resolution infrared camera, and a microphone.
The recording system was implemented using a parallel programming model to maximize the data throughput.

The recording system was developed as the product of this thesis.
This thesis documents the used sensors,
architecture of the implemented system,
and the data formats produced by the system.
The developed system was slightly unstable,
but sufficient for recording small-scale data sets.
The instability was likely resulting from either the used radar device or some programming error in the implemented system.
If the instability is fixed,
the system will also be capable of recording larger-scale data sets.
Nevertheless, the implemented system will serve as a good basis for future research and development.