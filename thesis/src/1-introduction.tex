As the power of computers grows and capability of sensors increases,
more and more incredible things are becoming possible.
One such thing is \gls{har},
for which the applications are many.

Not only is it possible to detect when people are running or walking~\cite{running-walking},
but even respiratory rates~\cite{breathing}, falling down~\cite{falling}, shoplifting~\cite{shoplifting},
operating room procedures~\cite{operatingroom}, hand gestures~\cite{handgestures},
and certain illnesses~\cite{parkinsson} have been proven to be recognizable by sensors.
The possibilities seem almost limitless.

For long, the focus in \gls{har} research has been in wearable sensors and visible spectrum imaging (RGB video).
Wearable sensors are a well-established mechanism for human activity recognition, that is already being used in multiple commercial applications,
such as excercise and sleep recognition in smart watches~\cite{wearables}.
While wearable sensors are undeniably a robust means of activity recognition,
they can be cumbersome and unfashionable, require willful equipping, and rarely can be connected to mains.
In \gls{har}, RGB video has been a staple of remote sensing for a long time.
Unfortunately it is very prone to occlusion and lighting conditions.
In addition, there are concerns about the privacy issues related to visible spectrum imaging.~\cite{sensing-survey}

In addition to visible spectrum imaging,
remote sensors include depth imaging (stereo camera),
infrared imaging, acoustic sensing (microphones) and electromagnetic sensing (e.g. radar).
Different kinds of proximity sensors, such as magnetic switches, pressure sensors, temperature sensors,
and electrostatic proximity sensors can also be used,
although their installation may be more labour-intensive~\cite{sensing-survey}.

It has been shown that depth imaging can achieve at least similar performance in activity recognition as visible spectrum imaging,
while simultaneously preserving privacy~\cite{depth}.
In complex scenes, using depth imaging can increase the performance of activity recognition substantially~\cite{depth-2}.
Depth imaging still suffers from many of the same problems as visible spectrum imaging:
most importantly obstruction.

Radar devices are capable of sensing through visual obstructions
and have been demonstrated to achieve very good performance at \gls{har},
especially when operating on the \gls{mmWave} spectrum.
Another upside for radar imaging is that unlike cameras, 
they are not susceptible to lighting conditions.~\cite{radar-survey}
As a downside, they are active devices in the sense that they must also have an active transmitter.
Although passive radars exist, the sensing performance is compromised.
WiFi signals and channel state information have also been demonstrated to be an effective alternative to radar sensing
in environments where WiFi is available~\cite{sensing-survey}.

Another alternative for visible spectrum imaging is the infrared camera.
Because the measured quantity is emitted directly from the human body,
it is capable of operating even in zero-light conditions where a normal visible-spectrum camera would fail.
It has been demonstrated that even as low as $8 \times 8$ pixel resolution infrared camera can be used
to detect a limited set of simple activities with high accuracy~\cite{ir-simple-activities}.

Sensor fusion can be leveraged to provide extra context information for improving human activity recognition~\cite{fusion1, fusion2}.
With some additional context information,
even the low resolution infrared camera is capable of recognizing more complex household activities~\cite{ir-household}.
By combining multiple remote sensing modalities,
it is expected that the activity detection accuracy can be increased by a large margin.

Machine learning is one of the key technologies used for \gls{har}.
For training machine learning models, it is necessary to have large quantities of data available.
Unfortunately, the number of data sets that considers sensor fusion in \gls{har} context is quite low~\cite{sensing-survey}.
The number of available data sets is brought even lower when only the data sets that consider remote sensors are included.

Remote sensing will certainly play a big role in the future of \gls{har}.
Therefore, it was considered valuable to create a portable multi-modal remote sensing system that can be used for recording data sets.
In this thesis, such a system was created.
The sensors installed in the systems were a combined visible and depth spectrum camera,
an $8 \times 8$ pixel infrared camera, a $4 \times 4$ channel microphone and a 60 GHz radar.

The created system is a key enabler of research considering sensor fusion and remote sensing in \gls{har}.
Due to the portable and low-labour install of the system,
it can be used to record data sets in various environments and with multiple data modalities with ease.
Surely, the data sets that will be recorded with the system will push forward the boundaries of \gls{har}.

Chapter \ref{ch:2-premise} will describe the sensors used in the system in more detail and establish requirements for the developed system.
In Chapter \ref{ch:3-system}, the implementation of the system will be detailed on an architectural level.
Details of the source code will not be discussed. The source code of the project is available for public viewing on GitHub~\cite{github-link}.
The data formats produced by the recording system will be documented in Chapter \ref{ch:4-files-and-post} along some data processing examples.
The quality and performance of the system will be briefly discussed in Chapter \ref{ch:5-evaluation},
and finally, Chapter \ref{ch:6-conclusion} will conclude the thesis.